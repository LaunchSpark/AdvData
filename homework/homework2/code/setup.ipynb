{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2 \u2013 Natural Language Processing\n",
    "\n",
    "This notebook follows the homework questions sequentially using the files in `homework/homework2/resources` and adapts the workflow from `textbook code.txt`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "Install/import required libraries and point to the resources folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet pandas matplotlib seaborn nltk scikit-learn\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk.tree import Tree\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "RESOURCES = Path(\"../resources\")\n",
    "RESOURCES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK resources used for tokenization/tagging/chunking\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Word Tagging (50 points)\n",
    "\n",
    "### Q1.1 Parse the sentences of the speech.\n",
    "Using `I-have-a-dream-speech.txt`, tokenize the speech into sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load speech text\n",
    "speech_path = RESOURCES / \"I-have-a-dream-speech.txt\"\n",
    "speech = speech_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Parse sentences\n",
    "sentences = nltk.sent_tokenize(speech)\n",
    "\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(\"\\nFirst 3 sentences:\")\n",
    "for i, s in enumerate(sentences[:3], start=1):\n",
    "    print(f\"{i}. {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Run parts-of-speech tagging to determine the top named entities.\n",
    "\n",
    "Following the textbook approach, we:\n",
    "1. tokenize each sentence,\n",
    "2. POS-tag each token,\n",
    "3. run `ne_chunk_sents(..., binary=True)`, and\n",
    "4. extract entity strings and remove obvious stop/common words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and POS tag\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(tokens) for tokens in tokenized_sentences]\n",
    "\n",
    "# Named entity chunking (binary NE chunker)\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "\n",
    "def get_entity_names(tree):\n",
    "    \"\"\"Recursively extract named entities from an NLTK chunk tree.\"\"\"\n",
    "    entity_names = []\n",
    "    if hasattr(tree, \"label\") and tree.label:\n",
    "        if tree.label() == \"NE\":\n",
    "            entity_names.append(\" \".join([child[0] for child in tree]))\n",
    "        else:\n",
    "            for child in tree:\n",
    "                entity_names.extend(get_entity_names(child))\n",
    "    return entity_names\n",
    "\n",
    "\n",
    "# Collect all entity names\n",
    "entity_names = []\n",
    "for tree in chunked_sentences:\n",
    "    entity_names.extend(get_entity_names(tree))\n",
    "\n",
    "# Basic cleanup: drop very short entities and common stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "filtered_entities = [\n",
    "    e.strip()\n",
    "    for e in entity_names\n",
    "    if len(e.strip()) > 1 and e.lower() not in stop_words\n",
    "]\n",
    "\n",
    "entity_counts = Counter(filtered_entities)\n",
    "top_10_entities = entity_counts.most_common(10)\n",
    "\n",
    "top_10_entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3 Plot Named Entities (X-axis) vs Frequency (Y-axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df = pd.DataFrame(top_10_entities, columns=[\"Named Entity\", \"Frequency\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(data=top_df, x=\"Named Entity\", y=\"Frequency\", palette=\"viridis\")\n",
    "ax.set_title(\"Top 10 Named Entities in 'I Have a Dream' Speech\")\n",
    "ax.set_xlabel(\"Named Entities\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.4 What do you notice about the top 10 named entities? Does anything surprise you?\n",
    "\n",
    "**Answer (rationale):**\n",
    "- The most frequent entities are expected to be strongly related to civil rights themes (for example, references to the U.S., states, and major identity groups).\n",
    "- Place names and national references tend to dominate because the speech repeatedly contrasts promises of American ideals with lived realities.\n",
    "- Depending on how NLTK chunks text, some entities may appear in slightly inconsistent forms (e.g., `United States` vs `America`) and this can split counts.\n",
    "- A possible surprise is that the chunker can classify some abstract or context-dependent words as entities; this is a known limitation of rule/statistical chunking without custom post-processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Tweets and LDA (50 points)\n",
    "\n",
    "### Q2.1 Using the provided training and testing tweets files, perform LDA.\n",
    "\n",
    "This section loads train/test tweets, vectorizes text, fits an LDA model, and inspects topic-word outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = RESOURCES / \"Train_QuantumTunnel_Tweets.csv\"\n",
    "test_path = RESOURCES / \"Test_QuantumTunnel_Tweets.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"Train columns:\", train_df.columns.tolist())\n",
    "print(\"Test columns:\", test_df.columns.tolist())\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the expected text column exists\n",
    "text_col = \"Tweet\"\n",
    "assert text_col in train_df.columns and text_col in test_df.columns, \"Expected a 'Tweet' column in both files.\"\n",
    "\n",
    "# Vectorize tweet text for LDA\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    lowercase=True,\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    ")\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(train_df[text_col].astype(str))\n",
    "X_test_counts = vectorizer.transform(test_df[text_col].astype(str))\n",
    "\n",
    "# Fit LDA\n",
    "n_topics = 5\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    learning_method=\"batch\",\n",
    ")\n",
    "lda.fit(X_train_counts)\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_terms = [feature_names[i] for i in top_indices]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_terms)}\")\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda, feature_names, n_top_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic distribution for test tweets\n",
    "test_topic_dist = lda.transform(X_test_counts)\n",
    "test_topic_ids = test_topic_dist.argmax(axis=1)\n",
    "\n",
    "test_results = test_df.copy()\n",
    "test_results[\"Predicted_Topic\"] = test_topic_ids\n",
    "test_results[\"Topic_Confidence\"] = test_topic_dist.max(axis=1)\n",
    "\n",
    "test_results[[\"Tweet\", \"Predicted_Topic\", \"Topic_Confidence\"]].head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Show some predictions on test data. Does it look accurate?\n",
    "\n",
    "To make predictions interpretable, we map each topic to a rough human label based on top words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL supervised check (if Data_Science label exists):\n",
    "# Use train labels to estimate practical prediction quality.\n",
    "if \"Data_Science\" in train_df.columns and \"Data_Science\" in test_df.columns:\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train_counts, train_df[\"Data_Science\"])\n",
    "    y_pred = clf.predict(X_test_counts)\n",
    "    acc = accuracy_score(test_df[\"Data_Science\"], y_pred)\n",
    "\n",
    "    print(f\"Baseline classification accuracy (LogReg on BoW): {acc:.3f}\")\n",
    "    print(classification_report(test_df[\"Data_Science\"], y_pred))\n",
    "else:\n",
    "    print(\"No ground-truth label column found in both files for supervised accuracy check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (rationale):**\n",
    "- The LDA topic assignments generally look plausible for broad themes (e.g., data/science/programming vs general conversation), but they are not perfect on short tweets.\n",
    "- Short texts are difficult for LDA because each tweet has limited context, slang, hashtags, and links, which can dilute topic quality.\n",
    "- If the optional supervised baseline is run, it usually gives a clearer estimate of predictive accuracy than unsupervised LDA topic IDs.\n",
    "- Overall, the outputs are useful for exploratory grouping, but I would not treat raw LDA topic IDs as high-precision class predictions without additional modeling/cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Final Notes\n",
    "- This notebook includes all required questions in order, with markdown answers and corresponding code blocks.\n",
    "- If any cell errors in a fresh environment, run setup/download cells first.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}